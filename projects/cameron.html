<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Cameron | Yuri Yasui</title>
  <link rel="stylesheet" href="../styles.css" />
</head>
<body>
  <header class="nav">
    <a class="logo" href="../index.html#top">Yuri Yasui</a>
    <nav>
      <a href="../index.html#about">About</a>
      <a href="../index.html#projects">Projects</a>
      <a href="../index.html#contact">Contact</a>
    </nav>
  </header>

  <main class="project-page">
    <!-- HERO -->
    <section class="project-hero">
      <div class="project-hero-inner">
        <p class="kicker">UC Berkeley AI Hackathon 2025 · 4-person team · 24h build</p>
        <h1>Cameron</h1>
        <p class="subtitle">
          誰でもカメラを「観察 → 理解 → 行動」できるAIエージェントに変える、ノーコード・スマートカメラフレームワーク。
        </p>

        <div class="badges">
          <span class="badge">Grand Prize（Creativity Track）</span>
          <span class="badge">Real-time Detection → Action</span>
          <span class="badge">No-code Rules</span>
        </div>

        <div class="cta">
          <a class="btn" href="https://devpost.com/software/cameron-9biv60" target="_blank" rel="noreferrer">Devpost</a>
          <a class="btn ghost" href="https://github.com/" target="_blank" rel="noreferrer">GitHub（貼る）</a>
        </div>

        <p class="muted small">
          ※受賞情報・概要はDevpostに準拠（Winner GRAND PRIZE - Creativity Track）。  
        </p>
      </div>
    </section>

    <!-- OVERVIEW -->
    <section class="section">
      <h2>概要</h2>
      <p>
        防犯・介護・行動分析など、リアルタイムに人の動作を理解するスマートカメラの需要は高い一方で、
        既存製品は高価かつ設定が複雑で、個人や中小企業が導入しにくい課題がありました。
        Cameronは、専門知識がなくても「転倒したら通知」「一定時間動かなければ警告」などを
        <b>文章で指定するだけ</b>で実行できる、低コスト・高汎用・ノーコードな仕組みを目指しました。
      </p>
    </section>

    <!-- WHAT I BUILT -->
    <section class="section">
      <h2>何を作ったか</h2>
      <ul class="bullets">
        <li>ユーザーが文章で入力したルールを、実行可能な仕様（JSON）へ変換</li>
        <li>仕様に基づいて、映像からイベント（転倒・滞留など）をリアルタイム検知</li>
        <li>条件を満たしたら、指定手段（通知/メール/等）でアクション</li>
      </ul>
    </section>

    <!-- ARCHITECTURE -->
    <section class="section">
      <h2>バックエンド・ワークフロー（あなたの強みポイント）</h2>
      <div class="callout">
        <ol class="steps">
          <li><b>入力：</b>カメラ映像 + ユーザーの自然言語</li>
          <li><b>Gemini：</b>「検知対象 / トリガー / 閾値 / 通知方法」をJSONに構造化</li>
          <li><b>Real-time CV：</b>YOLOv8で検出 → OpenCVで追跡/特徴量 → ルール判定</li>
          <li><b>Action：</b>条件成立で、指定方法で通知</li>
        </ol>
      </div>

      <h3 class="mt">JSON仕様（例）</h3>
      <pre class="code"><code>{
  "object": "person",
  "event": "fall_detected",
  "threshold": { "min_seconds": 1.0 },
  "notify": { "type": "push", "message": "Fall detected" }
}</code></pre>
    </section>

    <!-- MY ROLE -->
    <section class="section">
      <h2>担当範囲</h2>
      <p>
        私は主に<b>バックエンドシステムとリアルタイム推論パイプライン</b>を担当し、
        YOLOv8 + OpenCV による物体検出・追跡・行動（イベント）判定の実装を行いました。
        限られた24時間の中で、動作するプロトタイプとして成立させることを最優先に設計・実装しました。
      </p>
    </section>

    <!-- WHY YOLO+OPENCV -->
    <section class="section">
      <h2>技術選定：なぜ YOLOv8 + OpenCV か</h2>

      <h3>YOLOv8を選んだ理由</h3>
      <ul class="bullets">
        <li>フレーム単位で高速推論でき、リアルタイム監視（転倒/滞留）に向いている</li>
        <li>短時間実装で「確実に動く」経験値があり、24時間開発に適していた</li>
      </ul>

      <h3>OpenCVをどう使ったか（“検出→イベント”変換）</h3>
      <ul class="bullets">
        <li>YOLOのBBoxだけでは「転倒」「滞留」など時間をまたぐ事象を定義しにくい</li>
        <li>OpenCVで追跡し、姿勢変化・速度・静止時間などの特徴量を作り if/then ルール化</li>
        <li>前処理/後処理も揃い、短時間で安定性と運用耐性を確保しやすい</li>
      </ul>

      <h3>Gemini Video Analysis と比較して</h3>
      <ul class="bullets">
        <li>スマートカメラ用途では<b>リアルタイム性・制御性・コスト・説明可能性</b>が重要</li>
        <li>YOLOv8はフレーム単位で処理でき、検知後すぐ通知できる</li>
        <li>判断根拠（どの特徴量で条件成立したか）を説明しやすい</li>
      </ul>
    </section>

    <!-- CHALLENGE -->
    <section class="section">
      <h2>一番大変だったことと解決</h2>
      <p>
        最大の課題は、<b>UX（誰でも使える）</b>と<b>リアルタイム解析（曖昧さNG）</b>の両立でした。
        初期はユーザーに検知対象や閾値を直接入力してもらう設計でしたが、専門知識が前提になりUXが悪化。
        一方で自然言語は曖昧なため、そのまま検知ロジックに使えません。
      </p>
      <p>
        そこで、Geminiを<b>自然言語→実行可能なJSON仕様</b>の変換役に固定し、
        JSONを境界としてYOLO+OpenCV側は<b>決定論的に検知</b>する設計に変更。
        この分離により、使いやすさと安定性を両立できました。
      </p>
    </section>

    <!-- LEARNINGS -->
    <section class="section">
      <h2>学び</h2>
      <ul class="bullets">
        <li>要件定義と役割分担：全員が迷わない“軸”を最初に作る</li>
        <li>MVP判断：精度や機能より「誰がどう使うか」を最優先する</li>
        <li>技術を社会課題の文脈に落とす：使われる形に設計する</li>
      </ul>

      <div class="callout">
        <p class="muted">
          Hackathon序盤はモデル精度や構成に寄りがちだったが、フィードバックを受けて
          「誰でも使えるノーコード」を軸に設計転換したことが、最終的な評価に繋がった。
        </p>
      </div>
    </section>

    <!-- OTHER TECH -->
    <section class="section">
      <h2>その他（補足：設計意図）</h2>
      <ul class="bullets">
        <li><b>Letta：</b>過去に設定した検知ルールを保存し、再利用できる体験を目指していた</li>
        <li><b>Groq：</b>GPU性能よりも「速さと一貫性」を重視した推論設計のため</li>
      </ul>
    </section>

    <footer class="footer">
      <p><a class="muted" href="../index.html">← Back to Projects</a></p>
    </footer>
  </main>
</body>
</html>
